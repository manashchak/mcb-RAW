{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3adb06",
   "metadata": {},
   "source": [
    "# RL\n",
    "\n",
    "## Defs:\n",
    "- **RL:** Take action to Maximize the the notion of cumulative reward (over time ?).\n",
    "- **Deep $Q$-Learning:** Extending RL by using a deep NN to predict the actions.\n",
    "\n",
    "## Plan:\n",
    "- **Linear $Q$-Net (DQN):** Feedforward NN with few linear layers.  \n",
    "\n",
    "<img src=\"image.png\" alt=\"SnakeGamePlan\" width= \"50%\" height=\"auto\"/>   \n",
    "\n",
    "- Rewards:\n",
    "  - Eat Food: $+10$.\n",
    "  - Game Over: $-10$.\n",
    "  - Else: $0$.\n",
    "- Actions:\n",
    "  - $[1, 0, 0]$ -> straight.\n",
    "  - $[0, 1, 0]$ -> right turn.\n",
    "  - $[0, 0, 1]$ -> left turn.\n",
    "  - Note: This is a First Person FOV movement style and this is better approach then a (left, right, up, down), because with (up, down) it is possible to take $180^0$ turn, which is not desirable for a Snake game.\n",
    "- States: (11 states)\n",
    "  - Note, if snake is already past the snake and might run into wall, then the one or more of the frist row would be 1.\n",
    "  - Note, for the direction rows, only one would be 1 and everything else would be zero, this depends on the desired movement of the snake.\n",
    "  - Note, for the food row, multiple could be 1, based on where the food is in relation to the snake.  \n",
    "   \n",
    "  <img src=\"image-1.png\" alt=\"SnakeGame_states\" width= \"50%\" height=\"auto\"/>   \n",
    "\n",
    "- Model: only 1 hidden layer.\n",
    "  - 11 different state inputs.\n",
    "  - 1 hidden layer of chosen size.\n",
    "  - 3 output numbers, higher number indicates that being 1 for the actions. So we could take max for this.\n",
    "\n",
    "  <img src=\"image-2.png\" alt=\"SnakeGame_model\" width= \"50%\" height=\"auto\"/>   \n",
    "\n",
    "- **(Deep) $Q$-learning:**\n",
    "  - Improve $Q$ Value = Quality of Actions.\n",
    "  - Pseudo Code:\n",
    "    1. Init $Q$-Value = (`init model`).\n",
    "    2. Choose action (`model.predict(state)`) or `random move`. (_Trade off between exploration vs exploitation_).\n",
    "    3. Perform action.\n",
    "    4. Measure Reward.\n",
    "    5. Update $Q$-value (+ _train model_).\n",
    "        - Update mechanism: _Bellman Equation_.  \n",
    "            <img src=\"image-3.png\" alt=\"SnakeGame_train_BellMan\" width= \"30%\" height=\"auto\"/>   \n",
    "        - Simplified:\n",
    "          - $Q$ = `model.predict(state_0)`.\n",
    "          - $Q_{new}$ = $R + \\gamma \\max{Q(state_{1})}$.\n",
    "          - $loss = (Q_{new} - Q)^{2}$.\n",
    "    6. Repeat 2-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588ae11",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
