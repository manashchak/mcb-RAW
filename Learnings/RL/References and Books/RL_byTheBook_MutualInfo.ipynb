{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14df821",
   "metadata": {},
   "source": [
    "## A Finite Markov Decision Process (MDP)\n",
    "\n",
    "Consider following diagram,\n",
    "\n",
    "- Time $t \\in \\{0,1,2,...\\}$ is descrete.\n",
    "- _UPPER CASE_ letters are random variables, and _lower case_ letters are values they can take.\n",
    "- $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}(s)$, $r \\in \\mathcal{R} \\in \\mathbb{R}$.\n",
    "- The dynamics of the agent-environment interaction is specified by a distribution fuction:\n",
    "  - $P(S_{t+1}=s^{'}, R_{t+1}=r | S_{t}=s, A_{t}=a)$. Short hand, $P(s^{'},r | s,a)$.\n",
    "  - This function gives probability of next state and next reward based on the current state and action.\n",
    "- The **Policy** in general also a prob distribution $\\Pi (a|s)$, but it's deterministic then we write $a=\\Pi(s)$.\n",
    "  - The goal of the policy is the maximum accumulation of rewards. We call the the **Return**\n",
    "- **Return**, $G_{t} = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_{k}$.\n",
    "  - Here $\\gamma$ is the discount parameters between 0~1. This is to weight future reward vs immediate reward.\n",
    "- **Goal**: select policy that will maximize, $max_{\\pi} \\mathbb{E}[G_{t}]$.\n",
    "\n",
    "Here is an example:   \n",
    "<img src=\"image-1.png\" alt=\"RL_math\" width = \"30%\" height=\"auto\">    \n",
    "\n",
    "- the goal is the choose a policy that will maximize the rewards, averaged over all/many trajectories.\n",
    "\n",
    "- **State Value Function:**\n",
    "  - $\\nu_{\\pi}(s) = \\mathbb{E}[G_{t}|S_{t}=s]$. Expectation of the return, given the current state.\n",
    "- **Action Value Function:**\n",
    "  - $q_{\\pi}(s,a) = \\mathbb{E}[G_{t}|S_{t}=s, A_{t}=a]$.\n",
    "- Optimal policy, $\\pi_{*}$ if $\\nu_{\\pi_{*}}(s) \\geq \\nu_{\\pi}(s)$, for all $s$ and for any $\\pi$.\n",
    "\n",
    "#### Assumptions:\n",
    "**Major Assumptions:**\n",
    "- The agent observes the state $S_{t}$ at each step.\n",
    "- The state $S_{t}$ is all you need to anticipate the future, this is called **Markov Property**.\n",
    "\n",
    "**Sometimes also assume:**\n",
    "- All states and actions can be listed in tabular fashion. Not possible for infinite combinations !\n",
    "- We have complete access to the environment's dynamics, meaning we know $p(s^{'}, r| s, a)$.\n",
    "\n",
    "## Dynamic Programing\n",
    "DP refers to algorithms used find the optimal policies which have _complete knowledge_ of the environment as an MDP.   \n",
    "In oher words, DP has acces to $P(s^{'}, r | s, a)$.\n",
    "\n",
    "## Bellman Equations\n",
    "- Consider we have states as ${s^{-2}, s^{-1}, s^{0}, s^{1}, s^{2}}$.\n",
    "- Let's the agent is sitting on state $s^{0}$. Agent can choose to go right or left, and for both direction below is the probability distribution to get corresponding rewards:   \n",
    "  <img src=\"image.png\" alt=\"RL_math\" width = \"30%\" height=\"auto\">    \n",
    "- Let's say from the policy distribution we have following probability of going left vs going right.\n",
    "  - going right: $\\pi (-> | s^{0})=0.6$.\n",
    "  - going left:  $\\pi (<- | s^{0})=0.4$.\n",
    "  - Discount factor $\\gamma = 0.95$.\n",
    "- Also assume, we know the state values of right states and left states.\n",
    "- Given the info above, _can we calculate the state value of the state $s^{0}$_? \n",
    "- **Yes! we can, through bellman equation**. _It connects state values_. This is how   \n",
    "<img src=\"image-2.png\" alt=\"bellman-1\" width = \"30%\" height=\"auto\">      \n",
    "    - _This means if we can solve some state values, we can solve all state values._\n",
    "\n",
    "## Bellman Optimality\n",
    "Recall,\n",
    "- Optimal state value function, $\\nu_{*}(s) = \\max_{\\pi} \\nu_{\\pi}(s)$.\n",
    "- Optimal action value function, $q_{*}(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$.\n",
    "Now,  \n",
    "- Given action values of taking right and taking left, can we determine the state value of current state ? \n",
    "- **Yes! using bellman optimality condition**   \n",
    "<img src=\"image-3.png\" alt=\"bellman-2\" width = \"30%\" height=\"auto\">      \n",
    "\n",
    "- Note, according to bellman equation setup, state value is the max of action values.\n",
    "- So the optimality condition is **Agent must always select highest action value.**\n",
    "\n",
    "### Bellman Building Blocks:\n",
    "For any policy $\\pi$, any state $s \\in \\mathcal{S}$, and any action $a \\in \\mathcal{A}$,\n",
    "1. $\\nu_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}(s)} \\pi(a|s)q_{\\pi}(s,a)$.\n",
    "   - Value of a state when following policy $\\pi$, is the policy weighted average of the action values.\n",
    "   - If we have 100 states, this will provide 100 equations.\n",
    "2. $q_{\\pi}(s,a) = \\sum_{s^{'} \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s^{'}, r | s,a) [r + \\gamma \\nu_{\\pi}(s^{'})]$.\n",
    "   - Action value of a state-action pair is the probability weighted average of the (reqward we'll get at the next step + one discounted value of the next state).\n",
    "\n",
    "<img src=\"image-4.png\" alt=\"bellman-3\" width = \"30%\" height=\"auto\">     \n",
    "\n",
    "- _Usually, Bellman equations creates a system of equations, and we usually need to solve this system of equation_.\n",
    "- We get **Bellman Optimality Equations** by making one simple change, _the optimal state value is the maximum of the action values_. Everything else in the image remains the same.\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c31fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
