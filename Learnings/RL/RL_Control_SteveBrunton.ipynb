{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fa7747",
   "metadata": {},
   "source": [
    "# RL meets Control Theory\n",
    "\n",
    "- Framework for learning how to interact with the environment from experience\n",
    "- Walking -> hard nonlinear control problem.   \n",
    "\n",
    "_Semi-Supervised Learning:_  Reward comes at the end. So it's a time-delayed label/reward.\n",
    "- **Agent:**\n",
    "  - Takes **Actions**, $\\alpha$ (_usually from a finite set of actions_).\n",
    "    - based on **Policy**, $\\Pi(s, \\alpha)$ = $P(a = \\alpha | S=s)$.\n",
    "    - Time dependent. takes actions at each time-step.\n",
    "  - Receives **Reward** $r$.\n",
    "- **Environment:**\n",
    "  - State $S$. Modeled as Markov decision process (MDP).\n",
    "    - If I took an action now, it has some probability that will take me from current state to some future state.\n",
    "  - Measure where actions took the agent to.\n",
    "    - Compute the value of each state. \n",
    "      - Sum of expected reward. $V_{\\Pi}(s) = \\mathbb{E}(\\sum_{t}\\gamma^{t}r_{t}|s_{0}=s)$.\n",
    "      - Discounted rate $\\gamma$. How much you want to get the reward right now vs the in future.\n",
    "- Goals:\n",
    "  - Optimize the policy to maximize the future rewards.\n",
    "  - **Dense Vs Sparse Rewards:**\n",
    "    - sparse rewards create challanges in policy optimization.\n",
    "  - **Sample Efficiency:**\n",
    "    - How many times I need to run the sim to get optimized policy.\n",
    "  - **Reward Shaping:**\n",
    "    - Expert design would give the RL model more curated dense reward in between the sparse rewards to shape or guide to the optimal policy.\n",
    "  - **Optimization:**\n",
    "    - Differential programming\n",
    "    - Monte Carlo\n",
    "    - Temporal Difference (model free)\n",
    "    - Bellman 1957\n",
    "    - Exploration vs. Exploitation.\n",
    "    - Policy Iteration. Iterative update the policy.\n",
    "    - Gradient Descent, Evolutionary optimization, Simulated Annealing.\n",
    "    - $Q$-Learning: learning value function and policy together.\n",
    "      - $Q(s,\\alpha)$= quality of state/action pair.\n",
    "      - $Q^{update}(s_t, a_t) = Q^{old}(s_t,a_t) + \\alpha (r_t + \\gamma \\max_{a}Q(s_{t+1},a)-Q^{old}(s_t,a_t))$.\n",
    "        - update = old + learning rate * quality difference \n",
    "    - Hindsight Replay:\n",
    "      - If I fail to reach desired state, I won't delete the events, intead I will save these event for future for some other reward. This way system learns more about the dynamics and physics of the environment.\n",
    "\n",
    "### _Credit Assignment Problem:_ \n",
    "TODO\n",
    "\n",
    "### Deep NN for Policy Optimization:\n",
    "- NN takes in parameters based on current state and expected rewards, and then learns _actions_ through NN network.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3c817",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
